# api.py - Complete Optimized Version with Fixed Variables
import os
import requests
import tempfile
import time
import logging
import re
import asyncio
from datetime import datetime
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from dotenv import load_dotenv
import urllib3
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor
import threading

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Light imports that work on all cloud platforms
import PyPDF2
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Configuration - OPTIMIZED FOR BETTER EXTRACTION
AUTH_TOKEN = "78b25ddaad17f4e8d85cde3dca81ade8319272062cf10b73ba148b425151f2fd"
MAX_RESPONSE_TIME = 25
MAX_CHUNK_SIZE = 1200    # Increased for better context
TOP_K_RETRIEVAL = 4      # Good balance
MAX_PAGES = 80           # Process more pages
TIMEOUT_PER_QUESTION = 4
DOWNLOAD_TIMEOUT = 15

# Authentication
auth_scheme = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(auth_scheme)):
    if credentials.scheme != "Bearer" or credentials.credentials != AUTH_TOKEN:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")
    return credentials.credentials

# Pydantic models
class QueryRequest(BaseModel):
    documents: str
    questions: List[str]

class QueryResponse(BaseModel):
    answers: List[str]

# Global variables - FIXED NAMING
vectorizer = None
llm = None  # Single LLM variable
app_start_time = time.time()
doc_cache = {}
cache_lock = threading.Lock()

def initialize_models():
    """Initialize AI models - FIXED"""
    global vectorizer, llm
    
    try:
        if vectorizer is None:
            logger.info("Loading optimized TF-IDF vectorizer...")
            vectorizer = TfidfVectorizer(
                max_features=3000,
                stop_words='english',
                ngram_range=(1, 3),  # Include 3-grams for better phrase matching
                max_df=0.85,
                min_df=1,
                lowercase=True,
                strip_accents='ascii'
            )
            logger.info("‚úÖ Vectorizer loaded")
        
        if llm is None:
            logger.info("Loading optimized Gemini Flash...")
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY not found")
            
            genai.configure(api_key=api_key)
            llm = genai.GenerativeModel(
                'gemini-1.5-flash',
                generation_config=genai.types.GenerationConfig(
                    temperature=0.05,  # Very low for consistent extraction
                    max_output_tokens=350,
                    top_p=0.9,
                    top_k=40
                )
            )
            logger.info("‚úÖ Optimized Gemini Flash loaded")
            
    except Exception as e:
        logger.error(f"‚ùå Model initialization failed: {e}")
        raise

def extract_text_from_pdf_improved(pdf_content):
    """Extract text from PDF with better structure preservation"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
            temp_file.write(pdf_content)
            temp_file_path = temp_file.name
        
        text_chunks = []
        full_text = ""
        
        with open(temp_file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = min(len(pdf_reader.pages), MAX_PAGES)
            
            logger.info(f"Processing {total_pages} pages from PDF")
            
            for page_num in range(total_pages):
                try:
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    
                    if text.strip():
                        # Clean up text better
                        text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
                        text = re.sub(r'([.!?])\s*', r'\1 ', text)  # Fix sentence spacing
                        full_text += f"\n--- Page {page_num + 1} ---\n{text}\n"
                        
                        # Create better chunks with overlap
                        sentences = re.split(r'(?<=[.!?])\s+', text)
                        current_chunk = ""
                        sentence_buffer = []
                        
                        for sentence in sentences:
                            sentence = sentence.strip()
                            if not sentence:
                                continue
                            
                            sentence_buffer.append(sentence)
                            test_chunk = " ".join(sentence_buffer)
                            
                            if len(test_chunk) <= MAX_CHUNK_SIZE:
                                current_chunk = test_chunk
                            else:
                                # Save current chunk
                                if current_chunk.strip():
                                    text_chunks.append({
                                        'text': current_chunk.strip(),
                                        'page': page_num + 1,
                                        'chunk_id': f"p{page_num + 1}_c{len(text_chunks)}"
                                    })
                                
                                # Start new chunk with overlap (keep last 2 sentences)
                                sentence_buffer = sentence_buffer[-2:] if len(sentence_buffer) > 2 else sentence_buffer
                                current_chunk = " ".join(sentence_buffer)
                        
                        # Add remaining chunk
                        if current_chunk.strip():
                            text_chunks.append({
                                'text': current_chunk.strip(),
                                'page': page_num + 1,
                                'chunk_id': f"p{page_num + 1}_c{len(text_chunks)}"
                            })
                            
                except Exception as e:
                    logger.warning(f"Error extracting page {page_num + 1}: {e}")
                    continue
        
        # Cleanup temp file
        try:
            os.unlink(temp_file_path)
        except:
            pass
            
        logger.info(f"‚úÖ Extracted {len(text_chunks)} text chunks")
        return text_chunks
        
    except Exception as e:
        logger.error(f"‚ùå PDF extraction failed: {e}")
        return []

def search_documents_improved(query, text_chunks):
    """Enhanced search with keyword boosting"""
    try:
        if not text_chunks:
            return []
        
        # TF-IDF search
        texts = [chunk['text'] for chunk in text_chunks]
        all_texts = texts + [query]
        
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        query_vector = tfidf_matrix[-1]
        document_vectors = tfidf_matrix[:-1]
        
        similarities = cosine_similarity(query_vector, document_vectors).flatten()
        
        # Enhanced keyword matching for insurance terms
        query_lower = query.lower()
        keyword_scores = []
        
        # More comprehensive insurance keywords
        keywords = {
            'grace period': 3.0, 'premium payment': 2.0, 'due date': 2.0,
            'waiting period': 3.0, 'pre-existing': 2.5, 'ped': 2.0,
            'maternity': 3.0, 'childbirth': 2.0, 'pregnancy': 2.0,
            'cataract': 3.0, 'surgery': 1.5,
            'organ donor': 3.0, 'donation': 2.0, 'transplant': 2.0,
            'no claim discount': 3.0, 'ncd': 2.5, 'discount': 1.5,
            'health check': 2.5, 'preventive': 2.0, 'check-up': 2.0,
            'hospital': 2.0, 'institution': 1.5, 'medical': 1.0,
            'ayush': 3.0, 'ayurveda': 2.0, 'yoga': 2.0, 'unani': 2.0,
            'room rent': 2.5, 'icu charges': 2.5, 'plan a': 2.0,
            'thirty days': 3.0, 'thirty-six': 3.0, '36 months': 3.0,
            'two years': 2.5, '24 months': 2.5, '5%': 2.0
        }
        
        for chunk in text_chunks:
            text_lower = chunk['text'].lower()
            score = 0
            
            # Keyword matching with weights
            for keyword, weight in keywords.items():
                if keyword in query_lower and keyword in text_lower:
                    score += weight
                    
            # Boost for exact number matches
            numbers = re.findall(r'\b\d+\s*(?:days?|months?|years?|%)\b', text_lower)
            if numbers and any(num in query_lower for num in numbers):
                score += 2.0
            
            # Query word coverage
            query_words = [w for w in query_lower.split() if len(w) > 2]
            found_words = sum(1 for word in query_words if word in text_lower)
            if query_words:
                score += (found_words / len(query_words)) * 1.0
            
            keyword_scores.append(score)
        
        # Combine scores
        combined_scores = []
        for i in range(len(text_chunks)):
            combined_score = similarities[i] * 2 + keyword_scores[i]  # Weight TF-IDF less
            combined_scores.append({
                'index': i,
                'score': combined_score,
                'tfidf_score': similarities[i],
                'keyword_score': keyword_scores[i]
            })
        
        # Sort and get top results
        combined_scores.sort(key=lambda x: x['score'], reverse=True)
        
        results = []
        for item in combined_scores[:TOP_K_RETRIEVAL]:
            if item['score'] > 0.1:
                chunk = text_chunks[item['index']]
                results.append({
                    'text': chunk['text'],
                    'score': float(item['score']),
                    'page': chunk['page'],
                    'chunk_id': chunk.get('chunk_id', f"chunk_{item['index']}")
                })
        
        # Fallback if no good results
        if not results:
            for i in range(min(TOP_K_RETRIEVAL, len(text_chunks))):
                chunk = text_chunks[i]
                results.append({
                    'text': chunk['text'],
                    'score': 0.1,
                    'page': chunk['page'],
                    'chunk_id': chunk.get('chunk_id', f"chunk_{i}")
                })
        
        logger.info(f"üîç Found {len(results)} relevant chunks (top score: {results[0]['score']:.2f})")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Search failed: {e}")
        return text_chunks[:TOP_K_RETRIEVAL] if text_chunks else []

def generate_answer_ultra_optimized(question, context_docs):
    """Ultra-optimized prompt for specific insurance policy extraction"""
    try:
        if not context_docs:
            return "No relevant information found in the document."
        
        # Use top 3 most relevant contexts
        best_contexts = context_docs[:3]
        context_parts = []
        
        for i, doc in enumerate(best_contexts):
            context_parts.append(f"[Extract {i+1} - Page {doc['page']}]:\n{doc['text']}")
        
        context = "\n\n".join(context_parts)
        
        # Ultra-specific prompt engineered for Gemini Flash
        prompt = f"""You are an insurance policy expert. Extract EXACT information from this policy document.

POLICY DOCUMENT:
{context}

QUESTION: {question}

CRITICAL INSTRUCTIONS:
1. Find SPECIFIC numbers: days, months, years, percentages, amounts
2. Quote EXACT policy language and terms
3. Include ALL conditions, limitations, and requirements
4. If you see "30 days", "36 months", "5% discount", "24 months" - use these PRECISE values
5. Look for specific definitions, waiting periods, coverage details
6. Do NOT say "not specified" if the information exists in the text
7. Be comprehensive but concise (150-250 words)

EXTRACT THE SPECIFIC POLICY DETAILS:"""

        response = llm.generate_content(prompt)
        answer = response.text.strip()
        
        # Quality check - if answer is too generic, try with just the best context
        generic_indicators = [
            "not specified", "not mentioned", "does not specify",
            "not explicitly", "cannot be determined", "not clear"
        ]
        
        if any(indicator in answer.lower() for indicator in generic_indicators) and len(best_contexts) > 1:
            logger.info("üîÑ Retrying with focused context...")
            
            focused_context = f"POLICY TEXT:\n{best_contexts[0]['text']}"
            
            focused_prompt = f"""{focused_context}

QUESTION: {question}

Find the EXACT answer with specific numbers, periods, and conditions from the policy text above:"""
            
            response = llm.generate_content(focused_prompt)
            answer = response.text.strip()
        
        return answer
        
    except Exception as e:
        logger.error(f"‚ùå Answer generation failed: {e}")
        return "Unable to generate answer due to an error."

def download_document_optimized(url):
    """Optimized document download"""
    logger.info(f"üì• Downloading policy document...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/pdf,*/*',
        'Connection': 'keep-alive'
    }
    
    try:
        response = requests.get(
            url, 
            headers=headers, 
            timeout=DOWNLOAD_TIMEOUT,
            verify=False, 
            stream=True
        )
        response.raise_for_status()
        
        content = b''
        max_size = 25 * 1024 * 1024  # 25MB limit
        
        for chunk in response.iter_content(chunk_size=32768):  # Larger chunks
            if chunk:
                content += chunk
                if len(content) > max_size:
                    logger.warning("‚ö†Ô∏è Document size limit reached, truncating")
                    break
                    
        logger.info(f"‚úÖ Downloaded {len(content):,} bytes successfully")
        return content
        
    except Exception as e:
        logger.error(f"‚ùå Download failed: {e}")
        raise

# Thread pool for any parallel operations
executor = ThreadPoolExecutor(max_workers=2)

# FastAPI App
app = FastAPI(
    title="HackRx 6.0 Policy Analyzer - ULTRA OPTIMIZED",
    description="Optimized API with enhanced prompts for precise policy extraction",
    version="2.5.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ Starting ULTRA-OPTIMIZED HackRx API...")
    try:
        initialize_models()
        logger.info("‚úÖ Ultra-optimized startup completed")
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        raise

@app.get("/")
async def root():
    uptime = time.time() - app_start_time
    return {
        "status": "‚ö° HackRx 6.0 ULTRA-OPTIMIZED Policy Analyzer Online",
        "version": "2.5.0",
        "uptime_seconds": round(uptime, 2),
        "models_loaded": vectorizer is not None and llm is not None,
        "optimizations": [
            "Ultra-specific prompts for Gemini Flash",
            "Enhanced insurance keyword matching",
            "Improved PDF extraction with overlap",
            "Quality-checked answer generation",
            "Focused context retry mechanism",
            "Insurance-domain specific search weights"
        ],
        "target": "Precise policy detail extraction",
        "endpoint": "/hackrx/run"
    }

@app.get("/health")
async def health_check():
    uptime = time.time() - app_start_time
    return {
        "status": "healthy",
        "uptime_seconds": round(uptime, 2),
        "models_loaded": vectorizer is not None and llm is not None,
        "google_api_configured": bool(os.getenv("GOOGLE_API_KEY")),
        "optimization_level": "ULTRA",
        "model": "gemini-1.5-flash-optimized"
    }

@app.post("/hackrx/run", response_model=QueryResponse)
async def run_analysis(request: QueryRequest, token: str = Depends(verify_token)):
    """Main analysis endpoint - ULTRA OPTIMIZED"""
    start_time = time.time()
    request_id = f"req_{int(start_time)}"
    
    logger.info(f"‚ö° [{request_id}] ULTRA-OPTIMIZED processing {len(request.questions)} questions")
    
    try:
        # Ensure models are loaded
        if vectorizer is None or llm is None:
            initialize_models()
        
        # Check document cache
        doc_hash = str(hash(request.documents))
        text_chunks = None
        
        with cache_lock:
            if doc_hash in doc_cache:
                text_chunks = doc_cache[doc_hash]
                logger.info(f"[{request_id}] üíæ Using cached document")
        
        if text_chunks is None:
            # Download document
            try:
                pdf_content = download_document_optimized(request.documents)
            except Exception as e:
                logger.error(f"[{request_id}] ‚ùå Download failed: {e}")
                fallback_answers = [
                    "Unable to access the policy document. Please verify the document URL is accessible."
                    for _ in request.questions
                ]
                return {"answers": fallback_answers}
            
            # Extract text with improved method
            text_chunks = extract_text_from_pdf_improved(pdf_content)
            if not text_chunks:
                logger.error(f"[{request_id}] ‚ùå No text extracted from PDF")
                fallback_answers = [
                    "Unable to extract readable text from the policy document."
                    for _ in request.questions
                ]
                return {"answers": fallback_answers}
            
            # Cache the processed document
            with cache_lock:
                doc_cache[doc_hash] = text_chunks
                # Keep cache manageable
                if len(doc_cache) > 3:
                    oldest_key = next(iter(doc_cache))
                    del doc_cache[oldest_key]
        
        logger.info(f"[{request_id}] üìã Processing with {len(text_chunks)} text chunks")
        
        # Process questions sequentially for accuracy
        answers = []
        for i, question in enumerate(request.questions, 1):
            try:
                logger.info(f"[{request_id}] üîç Q{i}: {question[:70]}...")
                
                # Search for relevant document sections
                relevant_docs = search_documents_improved(question, text_chunks)
                
                # Generate optimized answer
                answer = generate_answer_ultra_optimized(question, relevant_docs)
                answers.append(answer)
                
                logger.info(f"[{request_id}] ‚úÖ Q{i} completed (found {len(relevant_docs)} relevant sections)")
                
            except Exception as e:
                logger.error(f"[{request_id}] ‚ùå Q{i} failed: {e}")
                answers.append("An error occurred while processing this question.")
        
        total_time = time.time() - start_time
        logger.info(f"[{request_id}] üéâ ULTRA-OPTIMIZED PROCESSING COMPLETED in {total_time:.2f}s")
        
        return {"answers": answers}
        
    except Exception as e:
        logger.error(f"[{request_id}] ‚ùå Unexpected error: {e}")
        error_answers = [
            "An unexpected error occurred during processing."
            for _ in request.questions
        ]
        return {"answers": error_answers}

@app.get("/hackrx/run")
async def hackrx_info():
    """Endpoint information"""
    return {
        "message": "HackRx 6.0 ULTRA-OPTIMIZED Policy Document Analysis",
        "method": "POST",
        "endpoint": "/hackrx/run",
        "description": "Precision-engineered for insurance policy detail extraction",
        "features": [
            "Ultra-specific prompts for Gemini Flash",
            "Insurance domain keyword weighting",
            "Enhanced PDF text extraction",
            "Quality-checked response generation",
            "Focused context retry mechanism",
            "Optimized search and retrieval"
        ],
        "optimization_target": "Extract specific policy numbers, periods, and conditions",
        "expected_accuracy": "High precision for insurance policy questions"
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run("api:app", host="0.0.0.0", port=port, log_level="info")