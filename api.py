# api_optimized.py - ACCURACY-FOCUSED VERSION
import os
import requests
import tempfile
import time
import logging
import re
import asyncio
from datetime import datetime
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from dotenv import load_dotenv
import urllib3
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor
import threading

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Light imports that work on all cloud platforms
import PyPDF2
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# ACCURACY-FOCUSED CONFIGURATION
AUTH_TOKEN = "78b25ddaad17f4e8d85cde3dca81ade8319272062cf10b73ba148b425151f2fd"
MAX_RESPONSE_TIME = 25
MAX_CHUNK_SIZE = 800      # Smaller chunks for better precision
TOP_K_RETRIEVAL = 6       # More chunks for better recall
MAX_PAGES = 100           # Process more pages for completeness
TIMEOUT_PER_QUESTION = 4
DOWNLOAD_TIMEOUT = 15

# Authentication
auth_scheme = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(auth_scheme)):
    if credentials.scheme != "Bearer" or credentials.credentials != AUTH_TOKEN:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")
    return credentials.credentials

# Pydantic models
class QueryRequest(BaseModel):
    documents: str
    questions: List[str]

class QueryResponse(BaseModel):
    answers: List[str]

# Global variables
vectorizer = None
llm = None
app_start_time = time.time()
doc_cache = {}
cache_lock = threading.Lock()

def initialize_models():
    """Initialize AI models with accuracy focus"""
    global vectorizer, llm
    
    try:
        if vectorizer is None:
            logger.info("Loading accuracy-focused TF-IDF vectorizer...")
            vectorizer = TfidfVectorizer(
                max_features=5000,      # More features for better matching
                stop_words='english',
                ngram_range=(1, 4),     # Include 4-grams for better phrase matching
                max_df=0.8,
                min_df=1,
                lowercase=True,
                strip_accents='ascii',
                token_pattern=r'(?u)\b[A-Za-z0-9][A-Za-z0-9\-\.]*[A-Za-z0-9]\b|\b[A-Za-z0-9]\b'  # Better number handling
            )
            logger.info("‚úÖ Accuracy-focused vectorizer loaded")
        
        if llm is None:
            logger.info("Loading Gemini with accuracy settings...")
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY not found")
            
            genai.configure(api_key=api_key)
            llm = genai.GenerativeModel(
                'gemini-1.5-flash',
                generation_config=genai.types.GenerationConfig(
                    temperature=0.0,        # Zero temperature for consistency
                    max_output_tokens=400,
                    top_p=0.8,
                    top_k=30
                )
            )
            logger.info("‚úÖ Accuracy-focused Gemini loaded")
            
    except Exception as e:
        logger.error(f"‚ùå Model initialization failed: {e}")
        raise

def extract_text_with_structure(pdf_content):
    """Enhanced text extraction with better structure preservation"""
    try:
        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
            temp_file.write(pdf_content)
            temp_file_path = temp_file.name
        
        text_chunks = []
        
        with open(temp_file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = min(len(pdf_reader.pages), MAX_PAGES)
            
            logger.info(f"Processing {total_pages} pages for accuracy")
            
            for page_num in range(total_pages):
                try:
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    
                    if text.strip():
                        # Better text cleaning for accuracy
                        text = re.sub(r'\s+', ' ', text)
                        text = re.sub(r'([.!?])\s*', r'\1 ', text)
                        
                        # Preserve important formatting patterns
                        text = re.sub(r'(\d+)\s*days?', r'\1 days', text, flags=re.IGNORECASE)
                        text = re.sub(r'(\d+)\s*months?', r'\1 months', text, flags=re.IGNORECASE)
                        text = re.sub(r'(\d+)\s*years?', r'\1 years', text, flags=re.IGNORECASE)
                        text = re.sub(r'(\d+)\s*%', r'\1%', text)
                        
                        # Split into smaller, more precise chunks
                        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
                        current_chunk = ""
                        
                        for sentence in sentences:
                            sentence = sentence.strip()
                            if not sentence:
                                continue
                            
                            test_chunk = f"{current_chunk} {sentence}".strip()
                            
                            if len(test_chunk) <= MAX_CHUNK_SIZE:
                                current_chunk = test_chunk
                            else:
                                # Save current chunk
                                if current_chunk.strip():
                                    text_chunks.append({
                                        'text': current_chunk.strip(),
                                        'page': page_num + 1,
                                        'chunk_id': f"p{page_num + 1}_c{len(text_chunks)}"
                                    })
                                
                                # Start new chunk
                                current_chunk = sentence
                        
                        # Add remaining chunk
                        if current_chunk.strip():
                            text_chunks.append({
                                'text': current_chunk.strip(),
                                'page': page_num + 1,
                                'chunk_id': f"p{page_num + 1}_c{len(text_chunks)}"
                            })
                            
                except Exception as e:
                    logger.warning(f"Error extracting page {page_num + 1}: {e}")
                    continue
        
        # Cleanup
        try:
            os.unlink(temp_file_path)
        except:
            pass
            
        logger.info(f"‚úÖ Extracted {len(text_chunks)} chunks for accuracy")
        return text_chunks
        
    except Exception as e:
        logger.error(f"‚ùå PDF extraction failed: {e}")
        return []

def enhanced_search(query, text_chunks):
    """Enhanced search with multiple ranking signals"""
    try:
        if not text_chunks:
            return []
        
        # TF-IDF baseline
        texts = [chunk['text'] for chunk in text_chunks]
        all_texts = texts + [query]
        
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        query_vector = tfidf_matrix[-1]
        document_vectors = tfidf_matrix[:-1]
        
        tfidf_similarities = cosine_similarity(query_vector, document_vectors).flatten()
        
        # Enhanced keyword matching with insurance domain knowledge
        query_lower = query.lower()
        enhanced_keywords = {
            # Time periods - highest priority
            'grace period': 4.0, 'thirty days': 4.0, '30 days': 4.0,
            'waiting period': 4.0, 'thirty-six': 4.0, '36 months': 4.0,
            'two years': 3.5, '24 months': 3.5, '2 years': 3.5,
            
            # Medical conditions and procedures
            'pre-existing': 3.5, 'ped': 3.0, 'cataract': 4.0,
            'maternity': 4.0, 'childbirth': 3.0, 'pregnancy': 3.0,
            'organ donor': 4.0, 'transplant': 3.0,
            
            # Policy features
            'no claim discount': 4.0, 'ncd': 3.5, '5%': 3.0,
            'health check': 3.5, 'preventive': 3.0,
            'room rent': 3.5, 'icu charges': 3.5,
            
            # Definitions and coverage
            'hospital': 3.0, 'institution': 2.5, '10 beds': 3.5,
            'ayush': 4.0, 'ayurveda': 3.0, 'yoga': 3.0,
            
            # Plan specifics
            'plan a': 3.5, '1%': 3.0, '2%': 3.0,
            
            # General insurance terms
            'premium': 2.0, 'policy': 1.5, 'coverage': 2.0,
            'medical expenses': 2.5, 'treatment': 2.0
        }
        
        # Calculate enhanced scores
        combined_scores = []
        for i, chunk in enumerate(text_chunks):
            text_lower = chunk['text'].lower()
            
            # Keyword score
            keyword_score = 0
            for keyword, weight in enhanced_keywords.items():
                if keyword in query_lower and keyword in text_lower:
                    # Boost for exact matches in both query and text
                    keyword_score += weight
                    # Additional boost for multiple occurrences
                    occurrences = text_lower.count(keyword)
                    if occurrences > 1:
                        keyword_score += weight * 0.3
            
            # Exact number matching (critical for insurance)
            query_numbers = re.findall(r'\b\d+\s*(?:days?|months?|years?|%)\b', query_lower)
            text_numbers = re.findall(r'\b\d+\s*(?:days?|months?|years?|%)\b', text_lower)
            
            number_score = 0
            for qnum in query_numbers:
                for tnum in text_numbers:
                    if qnum == tnum:
                        number_score += 3.0  # High boost for exact number matches
            
            # Question word coverage
            query_words = [w for w in query_lower.split() if len(w) > 2 and w not in ['the', 'and', 'for', 'are', 'this', 'that']]
            found_words = sum(1 for word in query_words if word in text_lower)
            coverage_score = (found_words / len(query_words)) * 1.5 if query_words else 0
            
            # Position bonus (early chunks might be more important)
            position_score = max(0, (len(text_chunks) - i) / len(text_chunks)) * 0.5
            
            # Length penalty for very short chunks
            length_penalty = 0 if len(chunk['text']) < 50 else 0
            
            # Combined score
            final_score = (
                tfidf_similarities[i] * 1.5 +      # TF-IDF base
                keyword_score +                     # Keyword matching
                number_score +                      # Exact number matching
                coverage_score +                    # Word coverage
                position_score -                    # Position bonus
                length_penalty                      # Length penalty
            )
            
            combined_scores.append({
                'index': i,
                'score': final_score,
                'tfidf': float(tfidf_similarities[i]),
                'keyword': keyword_score,
                'number': number_score,
                'coverage': coverage_score
            })
        
        # Sort by combined score
        combined_scores.sort(key=lambda x: x['score'], reverse=True)
        
        # Select top chunks with score threshold
        results = []
        for item in combined_scores[:TOP_K_RETRIEVAL]:
            if item['score'] > 0.2:  # Minimum relevance threshold
                chunk = text_chunks[item['index']]
                results.append({
                    'text': chunk['text'],
                    'score': float(item['score']),
                    'page': chunk['page'],
                    'chunk_id': chunk.get('chunk_id', f"chunk_{item['index']}"),
                    'debug': {
                        'tfidf': item['tfidf'],
                        'keyword': item['keyword'],
                        'number': item['number'],
                        'coverage': item['coverage']
                    }
                })
        
        # Fallback if no good results
        if not results:
            logger.warning("No high-scoring chunks found, using top TF-IDF results")
            top_indices = np.argsort(tfidf_similarities)[-TOP_K_RETRIEVAL:][::-1]
            for idx in top_indices:
                chunk = text_chunks[idx]
                results.append({
                    'text': chunk['text'],
                    'score': float(tfidf_similarities[idx]),
                    'page': chunk['page'],
                    'chunk_id': chunk.get('chunk_id', f"chunk_{idx}")
                })
        
        logger.info(f"üéØ Retrieved {len(results)} chunks (top score: {results[0]['score']:.2f})")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Search failed: {e}")
        return text_chunks[:TOP_K_RETRIEVAL] if text_chunks else []

def generate_precise_answer(question, context_docs):
    """Generate precise answers with better prompting"""
    try:
        if not context_docs:
            return "No relevant information found in the policy document."
        
        # Use top 4 most relevant contexts for better accuracy
        best_contexts = context_docs[:4]
        context_text = ""
        
        for i, doc in enumerate(best_contexts):
            context_text += f"\n--- POLICY EXCERPT {i+1} (Page {doc['page']}) ---\n{doc['text']}\n"
        
        # CRITICAL: Format check - ensure we return simple string answers
        prompt = f"""You are an expert insurance policy analyst. Extract PRECISE information from the policy document.

POLICY DOCUMENT EXCERPTS:
{context_text}

QUESTION: {question}

EXTRACTION RULES:
1. Find EXACT numbers: days, months, years, percentages, amounts
2. Quote PRECISE policy language and terms
3. Include ALL relevant conditions and limitations
4. Use the EXACT wording from the policy document
5. If specific timeframes like "30 days", "36 months", "2 years" exist, use them exactly
6. Be comprehensive but direct (aim for 100-200 words)
7. Do NOT say "not specified" if the information exists
8. Return ONLY the answer text - NO JSON, NO formatting, just the direct answer

ANSWER:"""

        response = llm.generate_content(prompt)
        answer = response.text.strip()
        
        # Clean up any unwanted formatting
        answer = re.sub(r'^ANSWER:\s*', '', answer, flags=re.IGNORECASE)
        answer = re.sub(r'^A:\s*', '', answer, flags=re.IGNORECASE)
        answer = re.sub(r'```.*?```', '', answer, flags=re.DOTALL)
        
        # Additional quality check for generic responses
        if len(answer) < 20 or any(phrase in answer.lower() for phrase in [
            "not mentioned", "not specified", "cannot determine", "not clear"
        ]):
            # Try with the most relevant chunk only
            logger.info("üîÑ Retrying with focused context...")
            focused_prompt = f"""POLICY TEXT:
{best_contexts[0]['text']}

QUESTION: {question}

Extract the specific answer with exact numbers and terms from the policy text above:"""
            
            focused_response = llm.generate_content(focused_prompt)
            answer = focused_response.text.strip()
        
        return answer
        
    except Exception as e:
        logger.error(f"‚ùå Answer generation failed: {e}")
        return "Unable to process this question due to an error."

def download_document_robust(url):
    """Robust document download with retries"""
    logger.info(f"üì• Downloading document...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/pdf,application/octet-stream,*/*',
        'Connection': 'keep-alive'
    }
    
    max_retries = 2
    for attempt in range(max_retries):
        try:
            response = requests.get(
                url, 
                headers=headers, 
                timeout=DOWNLOAD_TIMEOUT,
                verify=False, 
                stream=True
            )
            response.raise_for_status()
            
            content = b''
            max_size = 30 * 1024 * 1024  # 30MB limit
            
            for chunk in response.iter_content(chunk_size=32768):
                if chunk:
                    content += chunk
                    if len(content) > max_size:
                        logger.warning("‚ö†Ô∏è Document size limit reached")
                        break
                        
            logger.info(f"‚úÖ Downloaded {len(content):,} bytes")
            return content
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Download attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(1)  # Brief retry delay

# FastAPI App
app = FastAPI(
    title="HackRx 6.0 Policy Analyzer - ACCURACY OPTIMIZED",
    description="High-accuracy API optimized for precise policy information extraction",
    version="3.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ Starting ACCURACY-OPTIMIZED HackRx API...")
    try:
        initialize_models()
        logger.info("‚úÖ Accuracy-optimized startup completed")
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        raise

@app.get("/")
async def root():
    uptime = time.time() - app_start_time
    return {
        "status": "üéØ HackRx 6.0 ACCURACY-OPTIMIZED Policy Analyzer Online",
        "version": "3.0.0",
        "uptime_seconds": round(uptime, 2),
        "models_loaded": vectorizer is not None and llm is not None,
        "accuracy_optimizations": [
            "Enhanced multi-signal search ranking",
            "Smaller chunks for better precision",
            "Exact number matching for insurance terms",
            "Zero-temperature LLM for consistency",
            "Comprehensive keyword weighting",
            "Better text extraction and cleaning",
            "Focused retry mechanism for poor answers"
        ],
        "target": "Maximum accuracy for insurance policy Q&A",
        "endpoint": "/hackrx/run"
    }

@app.get("/health")
async def health_check():
    uptime = time.time() - app_start_time
    return {
        "status": "healthy",
        "uptime_seconds": round(uptime, 2),
        "models_loaded": vectorizer is not None and llm is not None,
        "google_api_configured": bool(os.getenv("GOOGLE_API_KEY")),
        "optimization_level": "ACCURACY-FOCUSED",
        "model": "gemini-1.5-flash-zero-temp"
    }

@app.post("/hackrx/run", response_model=QueryResponse)
async def run_analysis(request: QueryRequest, token: str = Depends(verify_token)):
    """Main analysis endpoint - ACCURACY OPTIMIZED"""
    start_time = time.time()
    request_id = f"acc_{int(start_time)}"
    
    logger.info(f"üéØ [{request_id}] ACCURACY-FOCUSED processing {len(request.questions)} questions")
    
    try:
        # Ensure models are loaded
        if vectorizer is None or llm is None:
            initialize_models()
        
        # Check cache
        doc_hash = str(hash(request.documents))
        text_chunks = None
        
        with cache_lock:
            if doc_hash in doc_cache:
                text_chunks = doc_cache[doc_hash]
                logger.info(f"[{request_id}] üíæ Using cached document")
        
        if text_chunks is None:
            # Download document
            try:
                pdf_content = download_document_robust(request.documents)
            except Exception as e:
                logger.error(f"[{request_id}] ‚ùå Download failed: {e}")
                fallback_answers = [
                    "Unable to access the policy document. Please verify the document URL is accessible."
                    for _ in request.questions
                ]
                return {"answers": fallback_answers}
            
            # Extract with enhanced structure preservation
            text_chunks = extract_text_with_structure(pdf_content)
            if not text_chunks:
                logger.error(f"[{request_id}] ‚ùå No text extracted")
                fallback_answers = [
                    "Unable to extract readable text from the policy document."
                    for _ in request.questions
                ]
                return {"answers": fallback_answers}
            
            # Cache with limit
            with cache_lock:
                doc_cache[doc_hash] = text_chunks
                if len(doc_cache) > 2:  # Keep cache small for accuracy
                    oldest_key = next(iter(doc_cache))
                    del doc_cache[oldest_key]
        
        logger.info(f"[{request_id}] üìä Processing with {len(text_chunks)} chunks")
        
        # Process questions with enhanced search
        answers = []
        for i, question in enumerate(request.questions, 1):
            try:
                logger.info(f"[{request_id}] Q{i}: {question[:60]}...")
                
                # Enhanced search
                relevant_docs = enhanced_search(question, text_chunks)
                
                # Generate precise answer
                answer = generate_precise_answer(question, relevant_docs)
                answers.append(answer)
                
                logger.info(f"[{request_id}] ‚úÖ Q{i} completed ({len(relevant_docs)} chunks, top score: {relevant_docs[0]['score']:.2f})")
                
            except Exception as e:
                logger.error(f"[{request_id}] ‚ùå Q{i} failed: {e}")
                answers.append("An error occurred while processing this question.")
        
        total_time = time.time() - start_time
        logger.info(f"[{request_id}] üéâ ACCURACY-OPTIMIZED PROCESSING COMPLETED in {total_time:.2f}s")
        
        return {"answers": answers}
        
    except Exception as e:
        logger.error(f"[{request_id}] ‚ùå Unexpected error: {e}")
        error_answers = [
            "An unexpected error occurred during processing."
            for _ in request.questions
        ]
        return {"answers": error_answers}

@app.get("/hackrx/run")
async def hackrx_info():
    """Endpoint information"""
    return {
        "message": "HackRx 6.0 ACCURACY-OPTIMIZED Policy Analyzer",
        "method": "POST",
        "endpoint": "/hackrx/run",
        "description": "Maximum accuracy for insurance policy Q&A",
        "accuracy_features": [
            "Multi-signal search ranking",
            "Enhanced keyword matching",
            "Exact number detection",
            "Zero-temperature consistency",
            "Comprehensive domain knowledge",
            "Quality-checked responses"
        ],
        "expected_improvement": "Significantly higher accuracy for insurance policy questions"
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    uvicorn.run("api:app", host="0.0.0.0", port=port, log_level="info")